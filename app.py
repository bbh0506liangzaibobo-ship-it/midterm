import streamlit as st
import tempfile
import os
import requests
import json
import numpy as np
from groq import Groq
from pypdf import PdfReader

# Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = Groq(api_key="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")

# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„í• 
def split_text(text, chunk_size=1000, chunk_overlap=200):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - chunk_overlap
    return chunks

# ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬
def process_uploaded_file(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    try:
        reader = PdfReader(tmp_file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() or ""
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        chunks = split_text(text)
        
        return chunks, text
    finally:
        os.unlink(tmp_file_path)

# ê°„ë‹¨í•œ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
def semantic_search(query, chunks):
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
    query_words = set(query.lower().split())
    scored_chunks = []
    
    for i, chunk in enumerate(chunks):
        chunk_words = set(chunk.lower().split())
        score = len(query_words.intersection(chunk_words))
        scored_chunks.append((score, chunk, i))
    
    # ê´€ë ¨ì„± ê¸°ì¤€ ì •ë ¬
    scored_chunks.sort(reverse=True)
    return [chunk for _, chunk, _ in scored_chunks[:3]]

# ë‹µë³€ ìƒì„±
def generate_response(query, relevant_chunks):
    context = "\n\n".join(relevant_chunks)
    
    prompt = f"""
    ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

    ë¬¸ì„œ ë‚´ìš©:
    {context}

    ì§ˆë¬¸: {query}

    ë‹µë³€:
    """
    
    try:
        chat_completion = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="llama-3.1-8b-instant",
            temperature=0.1,
            max_tokens=1024
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# Streamlit ì•±
def main():
    st.set_page_config(
        page_title="RAG ì±—ë´‡ - ì¤‘ê°„ê³ ì‚¬ í”„ë¡œì íŠ¸",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– RAG ì±—ë´‡ - ì¤‘ê°„ê³ ì‚¬ í”„ë¡œì íŠ¸")
    st.markdown("---")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "history" not in st.session_state:
        st.session_state.history = []
    if "chunks" not in st.session_state:
        st.session_state.chunks = None
    if "raw_text" not in st.session_state:
        st.session_state.raw_text = None
    
    # ì‚¬ì´ë“œë°” - ë¬¸ì„œ ì—…ë¡œë“œ
    with st.sidebar:
        st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_file = st.file_uploader(
            "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", 
            type=['pdf'],
            help="PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ AIê°€ í•´ë‹¹ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."
        )
        
        if uploaded_file and st.button("ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘"):
            with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                chunks, raw_text = process_uploaded_file(uploaded_file)
                st.session_state.chunks = chunks
                st.session_state.raw_text = raw_text
                st.success(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ({len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì¡°ê°)")
                
                # ì²˜ë¦¬ ì™„ë£Œ ë©”ì‹œì§€ ì¶”ê°€
                st.session_state.history.append({
                    'role': 'assistant', 
                    'content': f'ë¬¸ì„œ "{uploaded_file.name}"ì´(ê°€) ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!'
                })
        
        st.markdown("---")
        st.info("""
        **ğŸ“‹ ì‚¬ìš© ë°©ë²•:**
        1. ì™¼ìª½ì—ì„œ PDF ë¬¸ì„œ ì—…ë¡œë“œ
        2. 'ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘' ë²„íŠ¼ í´ë¦­
        3. ì•„ë˜ ì±„íŒ…ì°½ì—ì„œ ì§ˆë¬¸ ì…ë ¥
        4. AIê°€ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ì œê³µ
        
        **ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ:**
        - **Groq API**: ê³ ì† AI ëª¨ë¸
        - **PyPDF**: ë¬¸ì„œ ì²˜ë¦¬
        - **Streamlit**: ì›¹ ì¸í„°í˜ì´ìŠ¤
        """)
        
        # ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
        st.markdown("---")
        st.subheader("ì‹œìŠ¤í…œ ìƒíƒœ")
        if st.session_state.chunks:
            st.success("âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        else:
            st.warning("âš ï¸ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")

    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.subheader("ğŸ’¬ ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ")
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for msg in st.session_state.history:
        with st.chat_message(msg['role']):
            st.markdown(msg['content'])
    
    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ì§ˆë¬¸í•´ë³´ì„¸ìš”...")
    
    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.history.append({'role': 'user', 'content': user_input})
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # ì‘ë‹µ ìƒì„±
        if st.session_state.chunks:
            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
                    relevant_chunks = semantic_search(user_input, st.session_state.chunks)
                    # ë‹µë³€ ìƒì„±
                    response = generate_response(user_input, relevant_chunks)
                    
                    st.markdown(response)
                    st.session_state.history.append({'role': 'assistant', 'content': response})
                    
                    # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
                    with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                        st.write("ë‹¤ìŒ ë¬¸ì„œ ì¡°ê°ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤:")
                        for i, chunk in enumerate(relevant_chunks):
                            st.markdown(f"**ì°¸ê³  {i+1}:**")
                            st.text(chunk[:300] + "..." if len(chunk) > 300 else chunk)
                            st.markdown("---")
        else:
            warning_msg = "âš ï¸ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”!"
            st.warning(warning_msg)
            st.session_state.history.append({'role': 'assistant', 'content': warning_msg})

    # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
    if st.session_state.history and st.sidebar.button("ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°"):
        st.session_state.history = []
        st.rerun()

    # í‘¸í„°
    st.markdown("---")
    st.caption("ì¤‘ê°„ê³ ì‚¬ í”„ë¡œì íŠ¸ - RAG ì±—ë´‡ | Streamlit Cloud ë°°í¬")

if __name__ == "__main__":
    main()
